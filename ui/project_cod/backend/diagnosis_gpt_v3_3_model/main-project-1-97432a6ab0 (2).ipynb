{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14030084,"sourceType":"datasetVersion","datasetId":8934198}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers peft bitsandbytes trl accelerate datasets sentence-transformers rank_bm25 faiss-cpu scikit-learn pandas numpy\n\nprint(\"✅ Dependencies installed\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T04:11:47.201595Z","iopub.execute_input":"2025-12-08T04:11:47.201748Z","iopub.status.idle":"2025-12-08T04:13:16.936711Z","shell.execute_reply.started":"2025-12-08T04:11:47.201733Z","shell.execute_reply":"2025-12-08T04:13:16.935924Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.5/465.5 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m107.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m90.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m✅ Dependencies installed\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================\n# DiagnosisGPT v3.3 - Full updated notebook cell (drop-in)\n# - Paste and run in one cell.\n# =========================\n\nimport os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\nimport json, gc, random, re, pickle, time, math\nfrom tqdm.auto import tqdm\nimport numpy as np\nimport pandas as pd\nimport faiss\nimport torch\n\nfrom datasets import load_dataset, Dataset\nfrom sentence_transformers import SentenceTransformer\nfrom rank_bm25 import BM25Okapi\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, LogitsProcessorList\nfrom peft import LoraConfig, get_peft_model\n\ntorch.cuda.empty_cache(); gc.collect()\n\nprint(\"Device:\", \"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# --------------------\n# OPTIONS / FLAGS (edit as needed)\n# --------------------\nRUN_BUILD_RETRIEVER = True       # build KB + retriever\nRUN_BUILD_SYNTHETIC = True       # build / cache synthetic dataset\nRUN_BUILD_REAL = True            # build / cache real dataset (MedDialog)\nRUN_COMBINE = True               # combine synthetic + real into final dataset\nRUN_TRAIN = True                # set True only when you want to actually train (heavy)\nENABLE_LLM_SCORING = False       # if True, use LLM to compute candidate logprobs (slower)\nSAVE_TRANSCRIPT = True           # save JSON transcript of consultations\nEARLY_STOPPING_ENTROPY_EPS = 0.01  # if entropy decrease < eps across two rounds -> stop\nCACHE_DIR = \"/kaggle/working/cod_v3_3_cache\"\nos.makedirs(CACHE_DIR, exist_ok=True)\n\n# --------------------\n# CONFIG\n# --------------------\nMODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"   # change if needed\nTARGET_SYNTHETIC = 20000\nTARGET_REAL = 5000\nRETRIEVER_TOPK = 15\nALPHA_RETRIEVER = 0.6\nPER_DEVICE_BATCH = 1\nGRAD_ACCUM = 4\nTRAIN_MAX_STEPS = 100\nLEARNING_RATE = 2e-4\nLORA_R = 8\nLORA_ALPHA = 16\nLORA_DROPOUT = 0.05\nCONFIDENCE_THRESHOLD = 0.70\nINQUIRY_THRESHOLD = 0.50\nMAX_CONSULTATION_ROUNDS = 5\n\n# Paths\nKB_DF_PATH = os.path.join(CACHE_DIR, \"kb_df.pkl\")\nBM25_PATH = os.path.join(CACHE_DIR, \"bm25.pkl\")\nBM25_CORPUS_PATH = os.path.join(CACHE_DIR, \"bm25_corpus.pkl\")\nFAISS_INDEX_PATH = os.path.join(CACHE_DIR, \"faiss_index.bin\")\nDISEASE_SYMPTOM_MAP_PATH = os.path.join(CACHE_DIR, \"disease_symptom_map.pkl\")\n\nSYNTHETIC_PATH = os.path.join(CACHE_DIR, \"synthetic_data.jsonl\")\nREAL_PATH = os.path.join(CACHE_DIR, \"real_data.jsonl\")\nCOMBINED_PATH = os.path.join(CACHE_DIR, \"combined_data.jsonl\")\nFINAL_DATASET_DIR = os.path.join(CACHE_DIR, \"final_dataset\")\n\nMODEL_OUTPUT_DIR = \"diagnosis_gpt_v3_3\"\nFINAL_MODEL_DIR = os.path.join(MODEL_OUTPUT_DIR, \"model_final\")\n\nREAL_WORLD_CSV = \"/kaggle/input/meddialog-new2/meddialog_all_dialogues_all.csv\"  # update if needed\nTRANSCRIPT_SAVE_DIR = os.path.join(CACHE_DIR, \"consult_transcripts\")\nos.makedirs(TRANSCRIPT_SAVE_DIR, exist_ok=True)\n\n# Utility\ndef dedup(items):\n    seen = set()\n    out = []\n    for it in items:\n        if it not in seen:\n            out.append(it); seen.add(it)\n    return out\n\n# =========================\n# STEP A: Build / Load KB + Retriever (BM25 + FAISS + sentence-transformer)\n# =========================\nif RUN_BUILD_RETRIEVER:\n    if (\n        os.path.exists(KB_DF_PATH)\n        and os.path.exists(BM25_PATH)\n        and os.path.exists(BM25_CORPUS_PATH)\n        and os.path.exists(FAISS_INDEX_PATH)\n        and os.path.exists(DISEASE_SYMPTOM_MAP_PATH)\n    ):\n        print(\"Loading KB & retriever from cache...\")\n        kb_df = pd.read_pickle(KB_DF_PATH)\n        with open(DISEASE_SYMPTOM_MAP_PATH, \"rb\") as f:\n            disease_symptom_map = pickle.load(f)\n        with open(BM25_PATH, \"rb\") as f:\n            bm25 = pickle.load(f)\n        with open(BM25_CORPUS_PATH, \"rb\") as f:\n            tokenized_corpus = pickle.load(f)\n        index = faiss.read_index(FAISS_INDEX_PATH)\n        embedder = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\", device=\"cpu\")\n        print(\"KB & retriever loaded.\")\n    else:\n        print(\"Downloading Disease DB (FreedomIntelligence/Disease_Database) and building retriever...\")\n        kb_ds = load_dataset(\"FreedomIntelligence/Disease_Database\", \"en\", split=\"train\")\n        kb_df = pd.DataFrame(kb_ds)\n        if \"symptom_text\" not in kb_df.columns and \"common_symptom\" in kb_df.columns:\n            kb_df = kb_df.rename(columns={\"common_symptom\": \"symptom_text\"})\n        if \"symptom_text\" not in kb_df.columns:\n            possible = [c for c in kb_df.columns if \"symptom\" in c.lower()]\n            if possible:\n                kb_df = kb_df.rename(columns={possible[0]: \"symptom_text\"})\n            else:\n                kb_df[\"symptom_text\"] = \"\"\n        kb_df[\"symptom_text\"] = kb_df[\"symptom_text\"].fillna(\"\").astype(str)\n        if \"disease\" not in kb_df.columns:\n            if \"title\" in kb_df.columns:\n                kb_df = kb_df.rename(columns={\"title\": \"disease\"})\n            else:\n                kb_df[\"disease\"] = kb_df.index.astype(str)\n        kb_df[\"disease\"] = kb_df[\"disease\"].astype(str)\n\n        disease_symptom_map = {row[\"disease\"]: row[\"symptom_text\"].lower() for _, row in kb_df.iterrows()}\n\n        print(\"Building BM25 (tokenized corpus)...\")\n        tokenized_corpus = [re.findall(r\"\\w+\", str(doc).lower()) for doc in kb_df[\"symptom_text\"]]\n        bm25 = BM25Okapi(tokenized_corpus)\n\n        print(\"Loading embedder (all-mpnet-base-v2) and encoding KB...\")\n        embedder = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\", device=\"cpu\")\n        symptom_texts = kb_df[\"symptom_text\"].astype(str).tolist()\n        train_embeddings = embedder.encode(symptom_texts, convert_to_numpy=True, show_progress_bar=True, batch_size=32)\n        faiss.normalize_L2(train_embeddings)\n        dim = train_embeddings.shape[1]\n        index = faiss.IndexFlatIP(dim)\n        index.add(train_embeddings)\n\n        # cache\n        kb_df.to_pickle(KB_DF_PATH)\n        with open(DISEASE_SYMPTOM_MAP_PATH, \"wb\") as f:\n            pickle.dump(disease_symptom_map, f)\n        with open(BM25_PATH, \"wb\") as f:\n            pickle.dump(bm25, f)\n        with open(BM25_CORPUS_PATH, \"wb\") as f:\n            pickle.dump(tokenized_corpus, f)\n        faiss.write_index(index, FAISS_INDEX_PATH)\n        print(\"KB & retriever cached.\")\nelse:\n    if os.path.exists(KB_DF_PATH):\n        kb_df = pd.read_pickle(KB_DF_PATH)\n        with open(DISEASE_SYMPTOM_MAP_PATH, \"rb\") as f:\n            disease_symptom_map = pickle.load(f)\n        with open(BM25_PATH, \"rb\") as f:\n            bm25 = pickle.load(f)\n        with open(BM25_CORPUS_PATH, \"rb\") as f:\n            tokenized_corpus = pickle.load(f)\n        index = faiss.read_index(FAISS_INDEX_PATH)\n        embedder = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\", device=\"cpu\")\n        print(\"Loaded retriever from cache.\")\n    else:\n        raise RuntimeError(\"Retriever build skipped and cache not found. Set RUN_BUILD_RETRIEVER=True.\")\n\n# hybrid retrieve\ndef hybrid_retrieve(user_symptoms, k=5, alpha=ALPHA_RETRIEVER, negative_findings=None):\n    \"\"\"\n    Safe hybrid retriever that handles None, empty strings, and missing fields.\n    Prevents crashes like: AttributeError: 'NoneType' object has no attribute 'lower'\n    \"\"\"\n\n    # ---- SAFETY FIX (CRITICAL) ----\n    if user_symptoms is None:\n        user_symptoms = \"\"\n    user_symptoms = str(user_symptoms).strip().lower()\n    if user_symptoms == \"\":\n        user_symptoms = \"symptoms\"\n\n    if negative_findings is None:\n        negative_findings = []\n\n    # Tokenize safely\n    tokenized_query = re.findall(r\"\\w+\", user_symptoms)\n\n    # ---------- BM25 ----------\n    bm25_scores = np.array(bm25.get_scores(tokenized_query))\n    if bm25_scores.max() > 0:\n        bm25_scores /= (bm25_scores.max() + 1e-12)\n\n    # ---------- FAISS ----------\n    try:\n        user_emb = embedder.encode([user_symptoms], convert_to_numpy=True)\n        faiss.normalize_L2(user_emb)\n        sims, indices = index.search(user_emb, len(kb_df))\n\n        faiss_scores = np.zeros(len(kb_df))\n        faiss_scores[indices[0]] = sims[0]\n\n        if faiss_scores.max() > 0:\n            faiss_scores /= (faiss_scores.max() + 1e-12)\n    except Exception:\n        # fallback to BM25-only\n        faiss_scores = np.zeros(len(kb_df))\n\n    # Hybrid combine\n    hybrid_scores = alpha * faiss_scores + (1 - alpha) * bm25_scores\n    top_indices = np.argsort(hybrid_scores)[::-1]\n\n    # ---------- Collect candidates ----------\n    candidates = []\n    for i in top_indices:\n        disease = kb_df.iloc[i][\"disease\"]\n\n        skip = False\n        for neg in negative_findings:\n            if neg.lower() in disease_symptom_map.get(disease, \"\"):\n                skip = True\n                break\n\n        if not skip:\n            candidates.append(disease)\n\n        if len(candidates) >= k:\n            break\n\n    return dedup(candidates)[:k]\n\n\nprint(\"Hybrid retriever ready.\")\n\n# =========================\n# STEP B: Build Synthetic Dataset (CoD) -> upto TARGET_SYNTHETIC\n# =========================\nsynthetic_data = []\nif RUN_BUILD_SYNTHETIC:\n    if os.path.exists(SYNTHETIC_PATH):\n        print(\"Loading synthetic cache...\")\n        with open(SYNTHETIC_PATH, \"r\") as f:\n            for line in f:\n                synthetic_data.append(json.loads(line))\n        print(\"Loaded synthetic:\", len(synthetic_data))\n    else:\n        print(\"Building synthetic dataset from FreedomIntelligence/CoD-PatientSymDisease...\")\n        cod_ds = load_dataset(\"FreedomIntelligence/CoD-PatientSymDisease\", \"en\", split=\"train\")\n        shuffled_cod = cod_ds.shuffle(seed=42)\n\n        for entry in tqdm(shuffled_cod, desc=\"Parsing Synthetic\"):\n            if len(synthetic_data) >= TARGET_SYNTHETIC:\n                break\n            conversations = entry.get(\"CoD_conversations\")\n            if isinstance(conversations, str):\n                try:\n                    conversations = json.loads(conversations)\n                except Exception:\n                    continue\n            if not conversations:\n                continue\n\n            current_symptoms = []\n            negative_symptoms = []\n\n            for turn in conversations:\n                if turn.get(\"from\") == \"human\":\n                    if \"provided_symptom\" in turn and isinstance(turn[\"provided_symptom\"], list):\n                        for sym_pair in turn[\"provided_symptom\"]:\n                            if len(sym_pair) >= 2 and str(sym_pair[1]).lower() in (\"true\", \"1\", \"yes\"):\n                                current_symptoms.append(sym_pair[0])\n                            elif len(sym_pair) >= 2 and str(sym_pair[1]).lower() in (\"false\", \"0\", \"no\"):\n                                negative_symptoms.append(sym_pair[0])\n\n                elif turn.get(\"from\") == \"gpt\":\n                    dist = turn.get(\"confidence_assessment\", {}) or {}\n                    decision = turn.get(\"decision\", \"\")\n                    disease_recall = turn.get(\"disease_recall\", []) or []\n                    candidates = [d.get(\"disease\") for d in disease_recall if \"disease\" in d]\n                    if not candidates:\n                        continue\n\n                    if decision == \"diagnosis\":\n                        top_disease = max(dist, key=dist.get) if dist else \"Unknown\"\n                        action = {\"judge\": True, \"disease\": top_disease}\n                    else:\n                        question_text = turn.get(\"value\", \"\")\n                        action = {\"judge\": False, \"symptom\": question_text}\n\n                    # CoD++ richer output\n                    output_json = {\n                        \"step_1_symptom_abstraction\": {\n                            \"extracted_symptoms\": current_symptoms[:6],\n                            \"negative_findings\": negative_symptoms[:3]\n                        },\n                        \"step_2_candidate_recall\": {\n                            \"retrieved_diseases\": candidates[:10],\n                            \"retrieval_method\": \"Hybrid (BM25 + Dense Embedding)\"\n                        },\n                        \"step_3_diagnostic_reasoning\": {\n                            \"comparison\": f\"Comparing {len(candidates)} diseases with {len(current_symptoms)} symptoms\",\n                            \"top_3_candidates\": candidates[:3]\n                        },\n                        \"step_4_confidence_assessment\": {\n                            \"scores\": dist,\n                            \"max_confidence\": max(dist.values()) if dist else 0.0,\n                        },\n                        \"step_5_decision_making\": action\n                    }\n\n                    input_text = (\n                        f\"Patient Symptoms: {', '.join(current_symptoms)}.\\n\"\n                        f\"Negative Findings: {', '.join(negative_symptoms) if negative_symptoms else 'None'}.\\n\"\n                        f\"Candidates: {', '.join(candidates)}.\"\n                    )\n\n                    synthetic_data.append({\"input\": input_text, \"output\": json.dumps(output_json)})\n\n        # Save\n        with open(SYNTHETIC_PATH, \"w\") as f:\n            for row in synthetic_data:\n                f.write(json.dumps(row) + \"\\n\")\n        print(\"Saved synthetic:\", len(synthetic_data))\nelse:\n    if os.path.exists(SYNTHETIC_PATH):\n        with open(SYNTHETIC_PATH, \"r\") as f:\n            synthetic_data = [json.loads(l) for l in f]\n        print(\"Loaded synthetic:\", len(synthetic_data))\n    else:\n        print(\"No synthetic dataset available. Enable RUN_BUILD_SYNTHETIC to create it.\")\nprint(\"Synthetic dataset size:\", len(synthetic_data))\n\n# =========================\n# STEP C: Build Real-world Dataset (MedDialog CSV) using retriever-based symptom abstraction\n# =========================\nreal_data = []\nif RUN_BUILD_REAL:\n    if os.path.exists(REAL_PATH):\n        print(\"Loading real cache...\")\n        with open(REAL_PATH, \"r\") as f:\n            for line in f:\n                real_data.append(json.loads(line))\n        print(\"Loaded real:\", len(real_data))\n    else:\n        print(\"Loading MedDialog CSV:\", REAL_WORLD_CSV)\n        if not os.path.exists(REAL_WORLD_CSV):\n            print(\"Real-world CSV not found at path. Skipping real data creation.\")\n        else:\n            med_df = pd.read_csv(REAL_WORLD_CSV)\n            # Parse turns_json into patient / doctor\n            def parse_turns(turns_json):\n                try:\n                    turns = json.loads(turns_json)\n                    patient_text = \" \".join(t[\"utterance\"] for t in turns if \"patient\" in t.get(\"speaker\", \"\").lower())\n                    doctor_text = \" \".join(t[\"utterance\"] for t in turns if \"doctor\" in t.get(\"speaker\", \"\").lower())\n                    return patient_text.strip(), doctor_text.strip()\n                except Exception:\n                    return None, None\n\n            med_df[[\"src\", \"tgt\"]] = med_df[\"turns_json\"].apply(lambda x: pd.Series(parse_turns(x)))\n            med_df = med_df.dropna(subset=[\"src\", \"tgt\"]).reset_index(drop=True)\n\n            sample_n = min(TARGET_REAL, len(med_df))\n            med_df = med_df.sample(n=sample_n, random_state=42).reset_index(drop=True)\n\n            print(\"Preparing hybrid retrieval candidates for real data (batched)...\")\n            BATCH = 64\n            all_candidates = []\n            for i in tqdm(range(0, len(med_df), BATCH), desc=\"Batch Retrieval\"):\n                batch_texts = med_df[\"src\"].iloc[i : i + BATCH].tolist()\n                batch_candidates = [hybrid_retrieve(text, k=RETRIEVER_TOPK) for text in batch_texts]\n                all_candidates.extend(batch_candidates)\n\n            # Create real_data examples using retriever-based symptom abstraction\n            for src, tgt, candidates in zip(med_df[\"src\"].tolist(), med_df[\"tgt\"].tolist(), all_candidates):\n                symptoms = []\n                for c in candidates[:5]:\n                    st = disease_symptom_map.get(c, \"\")\n                    if st:\n                        parts = [s.strip() for s in re.split(r\"[;,\\n\\.]\", st) if s.strip()]\n                        symptoms.extend(parts)\n                symptoms = list(dict.fromkeys(symptoms))[:6]\n\n                input_text = (\n                    f\"Patient Symptoms: {', '.join(symptoms) if symptoms else src[:256]}.\\n\"\n                    f\"Negative Findings: None.\\n\"\n                    f\"Candidates: {', '.join(candidates[:5])}.\"\n                )\n\n                # Attempt to infer ground-truth disease from doctor's reply (very heuristic)\n                true_disease = next((c for c in candidates if c.lower() in tgt.lower()), \"Unknown\")\n                if true_disease != \"Unknown\":\n                    dist = {true_disease: 0.8}\n                    rem = 0.2 / (len(candidates) - 1) if len(candidates) > 1 else 0.0\n                    for cc in candidates:\n                        if cc != true_disease:\n                            dist[cc] = rem\n                else:\n                    dist = {c: 1/len(candidates) for c in candidates} if candidates else {}\n\n                output_json = {\n                    \"step_1_symptom_abstraction\": {\"extracted_symptoms\": symptoms},\n                    \"step_2_candidate_recall\": {\"retrieved_diseases\": candidates[:5]},\n                    \"step_3_diagnostic_reasoning\": {\"doctor_text_summary\": tgt[:400]},\n                    \"step_4_confidence_assessment\": {\"scores\": dist, \"max_confidence\": max(dist.values()) if dist else 0.0},\n                    \"step_5_decision_making\": {\"judge\": True, \"disease\": true_disease}\n                }\n\n                real_data.append({\"input\": input_text, \"output\": json.dumps(output_json)})\n\n            # Save\n            with open(REAL_PATH, \"w\") as f:\n                for row in real_data:\n                    f.write(json.dumps(row) + \"\\n\")\n            print(\"Saved real:\", len(real_data))\nelse:\n    if os.path.exists(REAL_PATH):\n        with open(REAL_PATH, \"r\") as f:\n            real_data = [json.loads(l) for l in f]\n        print(\"Loaded real:\", len(real_data))\n    else:\n        print(\"No real data available. Enable RUN_BUILD_REAL to create it.\")\n\nprint(\"Real dataset size:\", len(real_data))\n\n# =========================\n# STEP D: Merge Datasets (TARGET_SYNTHETIC synthetic + TARGET_REAL real)\n# =========================\nif RUN_COMBINE:\n    if os.path.exists(COMBINED_PATH):\n        print(\"Loading combined cache...\")\n        combined_data = [json.loads(l) for l in open(COMBINED_PATH)]\n    else:\n        print(\"Merging synthetic + real...\")\n        s = synthetic_data[:TARGET_SYNTHETIC]\n        r = real_data[:TARGET_REAL]\n        combined_data = s + r\n        random.shuffle(combined_data)\n        with open(COMBINED_PATH, \"w\") as f:\n            for row in combined_data:\n                f.write(json.dumps(row) + \"\\n\")\n        print(\"Saved combined:\", len(combined_data))\nelse:\n    if os.path.exists(COMBINED_PATH):\n        combined_data = [json.loads(l) for l in open(COMBINED_PATH)]\n        print(\"Loaded combined:\", len(combined_data))\n    else:\n        combined_data = synthetic_data + real_data\n        print(\"Combined created in-memory:\", len(combined_data))\n\nprint(\"Combined size:\", len(combined_data))\n\n# =========================\n# STEP E: Prepare HF Dataset for SFT (Alpaca-style prompt)\n# =========================\nalpaca_prompt = \"\"\"Below is a medical consultation example.\nFollow the 5-step diagnostic pipeline and output the JSON content exactly.\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"\n\ndef format_for_sft_batch(ex):\n    texts = []\n    for i, o in zip(ex[\"input\"], ex[\"output\"]):\n        texts.append(alpaca_prompt.format(i, o))\n    return {\"text\": texts}\n\nif os.path.exists(FINAL_DATASET_DIR):\n    final_dataset = Dataset.load_from_disk(FINAL_DATASET_DIR)\n    print(\"Loaded final_dataset from disk:\", len(final_dataset))\nelse:\n    final_dataset = Dataset.from_list(combined_data)\n    print(\"Created final Dataset object (unformatted). Samples:\", len(final_dataset))\n\n# =========================\n# STEP F: MODEL LOAD (4-bit) + LoRA (safe settings)  -- optional heavy\n# =========================\nprint(\"\\nLoading base model + LoRA (skipping heavy train if RUN_TRAIN=False)...\")\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n)\n\nmodel = None\ntokenizer = None\nif ENABLE_LLM_SCORING or RUN_TRAIN:\n    try:\n        model = AutoModelForCausalLM.from_pretrained(\n            MODEL_NAME,\n            quantization_config=bnb_config,\n            device_map=\"auto\",\n            trust_remote_code=True,\n        )\n        model.config.use_cache = False\n        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n        tokenizer.pad_token = tokenizer.eos_token\n\n        peft_config = LoraConfig(\n            r=LORA_R,\n            lora_alpha=LORA_ALPHA,\n            lora_dropout=LORA_DROPOUT,\n            bias=\"none\",\n            task_type=\"CAUSAL_LM\",\n            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n        )\n        model = get_peft_model(model, peft_config)\n        model.print_trainable_parameters()\n        print(\"Model & LoRA ready.\")\n    except Exception as e:\n        print(\"Could not fully load model. Error:\", e)\nelse:\n    print(\"Skipping model load (ENABLE_LLM_SCORING=False and RUN_TRAIN=False).\")\n\n# Format final dataset if tokenizer present (optional)\nif tokenizer is not None and not os.path.exists(FINAL_DATASET_DIR):\n    print(\"Formatting final HF dataset (this may take a while)...\")\n    final_dataset = final_dataset.map(format_for_sft_batch, batched=True, remove_columns=final_dataset.column_names)\n    final_dataset = final_dataset.map(lambda ex: {\"text\":[t + tokenizer.eos_token for t in ex[\"text\"]]}, batched=True)\n    final_dataset.save_to_disk(FINAL_DATASET_DIR)\n    print(\"Saved formatted final_dataset to disk.\")\nelif os.path.exists(FINAL_DATASET_DIR):\n    final_dataset = Dataset.load_from_disk(FINAL_DATASET_DIR)\n\nprint(\"Final_dataset size:\", len(final_dataset) if final_dataset is not None else \"N/A\")\n\n# =========================\n# STEP G (optional): TRAIN (SMOKE RUN)\n# =========================\nif RUN_TRAIN and model is not None and final_dataset is not None:\n    from trl import SFTTrainer, SFTConfig\n    import glob, os\n\n    os.makedirs(MODEL_OUTPUT_DIR, exist_ok=True)\n    sft_cfg = SFTConfig(\n        output_dir=MODEL_OUTPUT_DIR,\n        dataset_text_field=\"text\",\n        per_device_train_batch_size=PER_DEVICE_BATCH,\n        gradient_accumulation_steps=GRAD_ACCUM,\n        warmup_steps=30,\n        max_steps=TRAIN_MAX_STEPS,\n        learning_rate=LEARNING_RATE,\n        fp16=True,\n        logging_steps=10,\n        save_steps=200,\n        save_total_limit=2,\n        optim=\"paged_adamw_8bit\",\n        gradient_checkpointing=True,\n        gradient_checkpointing_kwargs={\"use_reentrant\": False},\n        report_to=\"none\",\n        packing=False,\n    )\n    trainer = SFTTrainer(model=model, train_dataset=final_dataset, args=sft_cfg)\n    torch.cuda.empty_cache(); gc.collect()\n    print(\"Starting smoke-run training (max_steps=%d) ...\" % TRAIN_MAX_STEPS)\n    trainer.train()\n    trainer.save_model(FINAL_MODEL_DIR)\n    tokenizer.save_pretrained(FINAL_MODEL_DIR)\n    print(\"Saved model to\", FINAL_MODEL_DIR)\nelse:\n    print(\"Skipping training step (RUN_TRAIN=False or model missing).\")\n\n# =========================\n# STEP H: Paper-aligned Interactive Consultation (improved)\n# =========================\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T05:09:39.034127Z","iopub.execute_input":"2025-12-08T05:09:39.034793Z","iopub.status.idle":"2025-12-08T05:22:43.193856Z","shell.execute_reply.started":"2025-12-08T05:09:39.034765Z","shell.execute_reply":"2025-12-08T05:22:43.192948Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nLoading KB & retriever from cache...\nKB & retriever loaded.\nHybrid retriever ready.\nLoading synthetic cache...\nLoaded synthetic: 20002\nSynthetic dataset size: 20002\nLoading real cache...\nLoaded real: 5000\nReal dataset size: 5000\nLoading combined cache...\nCombined size: 25000\nLoaded final_dataset from disk: 25000\n\nLoading base model + LoRA (skipping heavy train if RUN_TRAIN=False)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23adb46e2b4f45b590731da65ffffc1d"}},"metadata":{}},{"name":"stdout","text":"trainable params: 3,440,640 || all params: 7,619,057,152 || trainable%: 0.0452\nModel & LoRA ready.\nFinal_dataset size: 25000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Adding EOS to train dataset:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ac89b4710484c33b2e5533cf85a299f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing train dataset:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6478e3ea5ba426696717b818f3b5127"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating train dataset:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fba79abc2da041219a495cabe8b98df0"}},"metadata":{}},{"name":"stderr","text":"The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n","output_type":"stream"},{"name":"stdout","text":"Starting smoke-run training (max_steps=100) ...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 11:49, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>1.612100</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.431700</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.183600</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.842600</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.560100</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.518600</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.437800</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.370900</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.404600</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.401400</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Saved model to diagnosis_gpt_v3_3/model_final\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# ==============================\n# Updated InteractiveConsultationPaperV3\n# - Minimum 3 follow-ups\n# - Strong age/gender gating\n# - Red-flag detection\n# - Clean reasoning (Streamlit-friendly)\n# - Gemini integration (optional)\n# - Entropy-based follow-up selection\n# ==============================\n\nimport re, math, time, textwrap, os, numpy as np\n\n\n# -----------------------------------------------------\n# RED FLAG DETECTION\n# -----------------------------------------------------\ndef detect_red_flags(text):\n    RED_FLAGS = [\n        \"severe chest pain\", \"severe shortness of breath\", \"unable to breathe\",\n        \"blue lips\", \"cyanosis\", \"blood in sputum\", \"hemoptysis\",\n        \"confusion\", \"loss of consciousness\", \"fainting\", \"very low oxygen\",\n        \"spo2\", \"respiratory distress\"\n    ]\n    if not text:\n        return False\n    t = text.lower()\n\n    # simple phrase match\n    for rf in RED_FLAGS:\n        if rf in t:\n            return True\n\n    # SpO2 < 94 detection\n    m = re.search(r\"sp[o0]2[: ]*(\\d{2,3})\", t)\n    if m:\n        try:\n            if int(m.group(1)) < 94:\n                return True\n        except:\n            pass\n\n    return False\n\n\n# -----------------------------------------------------\n# GEMINI LLM NARRATIVE (optional)\n# -----------------------------------------------------\ndef call_external_narrative_llm(name, age, gender, symptoms, candidates, probs):\n    \"\"\"\n    Provides narrative reasoning using Gemini.\n    Requires environment var: GEMINI_API_KEY\n    Returns None on failure.\n    \"\"\"\n    try:\n        import google.generativeai as genai\n    except Exception:\n        return None\n\n    api_key = os.environ.get(\"GEMINI_API_KEY\")\n    if not api_key:\n        return None\n\n    try:\n        genai.configure(api_key=api_key)\n\n        prompt = f\"\"\"\nWrite a polished medical diagnostic reasoning narrative in 3–4 paragraphs:\n\nPatient Info:\n- Name: {name}\n- Age: {age}\n- Gender: {gender}\n\nReported Symptoms:\n{', '.join(symptoms)}\n\nCandidate Diseases:\n{', '.join(candidates)}\n\nProbability Scores:\n{probs}\n\nGuidelines:\n- Compare each disease briefly.\n- Match hallmark symptoms.\n- Mention age/gender incompatibility.\n- Explain why the final diagnosis is most likely.\nReturn plain text only.\n\"\"\"\n\n        model = genai.GenerativeModel(\"gemini-pro\")\n        resp = model.generate_content(prompt)\n\n        if hasattr(resp, \"text\") and resp.text:\n            return resp.text.strip()\n        return str(resp)\n\n    except Exception:\n        return None\n\n\n\n# -----------------------------------------------------\n#  Main Consultation Class\n# -----------------------------------------------------\nclass InteractiveConsultationPaperV3:\n\n    MIN_INQUIRIES_BEFORE_DIAG = 3   # <--- REQUIRED: at least 3 questions\n\n    def __init__(self,\n                 model=None,\n                 tokenizer=None,\n                 llm_scoring=False,\n                 device=None,\n                 k_candidates=6,\n                 k_symptoms_per_candidate=6,\n                 confidence_threshold=0.75,\n                 max_rounds=6,\n                 save_transcript=True):\n        \n        self.model = model\n        self.tokenizer = tokenizer\n        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        self.k = k_candidates\n        self.k_symptoms_per_candidate = k_symptoms_per_candidate\n        self.conf_threshold = confidence_threshold\n        self.max_rounds = max_rounds\n        self.llm_scoring = llm_scoring\n        self.save_transcript = save_transcript\n\n        self.S = []\n        self.negatives = set()\n        self.asked = set()\n        self.round = 0\n        self.transcript = {}\n\n    # -----------------------------------------------------\n    #  Utility Functions\n    # -----------------------------------------------------\n    def _softmax(self, score_map, temp=0.32):\n        keys = list(score_map.keys())\n        arr = np.array([score_map[k] for k in keys], float)\n        arr -= arr.max()\n        ex = np.exp(arr / temp)\n        ex /= (ex.sum() + 1e-12)\n        return {k: float(v) for k, v in zip(keys, ex)}\n\n    def _candidate_symptoms_pool(self, candidates):\n        pool = []\n        for d in candidates:\n            txt = disease_symptom_map.get(d, \"\") or \"\"\n            parts = [p.strip() for p in re.split(r\"[;,\\n\\.]\", txt) if p.strip()]\n            pool.extend(parts[: self.k_symptoms_per_candidate])\n        out, seen = [], set()\n        for p in pool:\n            lp = p.lower()\n            if lp not in seen and 1 <= len(p.split()) <= 8:\n                out.append(p); seen.add(lp)\n        return out\n\n    # -----------------------------------------------------\n    #  Strong Scoring + Age/Gender Gating\n    # -----------------------------------------------------\n    def _score_candidates(self, candidates, symptoms, age=None, gender=None):\n        s_text = \" \".join(symptoms).lower()\n        acute = bool(re.search(r\"\\b(day|days|acute|hours)\\b\", s_text))\n        chronic = bool(re.search(r\"\\b(month|months|year|chronic|long)\\b\", s_text))\n\n        # parse demographics\n        try:\n            age_i = int(age) if age else None\n        except:\n            age_i = None\n        g = gender.lower().strip() if gender else None\n\n        scores = {}\n\n        for i, d in enumerate(candidates):\n            dl = d.lower()\n            kb = (disease_symptom_map.get(d, \"\") or \"\").lower()\n\n            # -----------------------------------------\n            # HARD AGE/GENDER EXCLUSIONS\n            # -----------------------------------------\n            incompatible = False\n\n            if age_i is not None:\n                if age_i < 40 and any(x in dl for x in [\"elderly\", \"senile\", \"geriatric\"]):\n                    incompatible = True\n                if age_i >= 18 and any(x in dl for x in [\"pediatric\", \"child\", \"infant\"]):\n                    incompatible = True\n\n            if g:\n                if g.startswith(\"m\") and any(x in dl for x in [\"pregnan\", \"ovarian\", \"uterine\", \"breast\"]):\n                    incompatible = True\n                if g.startswith(\"f\") and any(x in dl for x in [\"prostate\", \"testicular\"]):\n                    incompatible = True\n\n            if incompatible:\n                scores[d] = 1e-12\n                continue\n\n            # -----------------------------------------\n            # MATCHING SCORE\n            # -----------------------------------------\n            exact = sum(1 for s in symptoms if s.lower() in kb)\n\n            partial = 0\n            for s in symptoms:\n                toks = re.findall(r\"\\w+\", s.lower())\n                hit = sum(1 for t in toks if t in kb)\n                if hit >= max(1, len(toks)//2):\n                    partial += 1\n\n            score = 1.0 + 2.0 * exact + 1.1 * partial\n            score += 0.22 * (self.k - i)  # small ranking bonus\n\n            # acute/chronic mismatch\n            if acute and \"chronic\" in dl:\n                score *= 0.5\n            if chronic and \"acute\" in dl:\n                score *= 0.65\n\n            # tumor penalty\n            if any(x in dl for x in [\"tumor\", \"malign\", \"carcinoma\", \"sarcoma\"]):\n                score *= 0.65\n\n            scores[d] = max(score, 1e-12)\n\n        return self._softmax(scores, temp=0.32)\n\n    # -----------------------------------------------------\n    #  Yes/No interpretation\n    # -----------------------------------------------------\n    def interpret_answer_as_bool(self, ans, symptom):\n        if not ans:\n            return False\n        a = ans.lower()\n        if a.startswith(\"y\"): return True\n        if a.startswith(\"n\"): return False\n        if \"day\" in a or \"week\" in a: return True\n        toks = re.findall(r\"\\w+\", symptom.lower())\n        return any(tok in a for tok in toks)\n\n    # -----------------------------------------------------\n    #  Entropy-based follow-up selection\n    # -----------------------------------------------------\n    def _simulate(self, candidates, S, symptom):\n        new = set(S); new.add(symptom)\n        probs = self._score_candidates(candidates, list(new))\n        s = sum(probs.values())\n        return {k: v/s for k, v in probs.items()}\n\n    def choose_inquiry_by_entropy(self, candidates):\n        base = self._score_candidates(candidates, list(self.S))\n        H0 = -sum(p * math.log(p + 1e-12) for p in base.values())\n\n        pool = self._candidate_symptoms_pool(candidates)\n        pool = [p for p in pool if p.lower() not in self.asked and p.lower() not in [s.lower() for s in self.S]]\n\n        best, best_delta = None, 0\n        for s in pool:\n            post = self._simulate(candidates, self.S, s)\n            H1 = -sum(p * math.log(p + 1e-12) for p in post.values())\n            delta = H0 - H1\n            if delta > best_delta:\n                best, best_delta = s, delta\n        return best\n\n    # -----------------------------------------------------\n    #  Final Report\n    # -----------------------------------------------------\n    def generate_final_cod_report(self, candidates, probs, symptoms,\n                                  name=None, age=None, gender=None, use_gemini=True):\n\n        sorted_probs = sorted(probs.items(), key=lambda x: x[1], reverse=True)\n        top_d, top_p = sorted_probs[0]\n\n        # Try Gemini\n        narrative = None\n        if use_gemini:\n            narrative = call_external_narrative_llm(\n                name, age, gender, list(symptoms),\n                [c for c, _ in sorted_probs[:5]],\n                {c: p for c, p in sorted_probs}\n            )\n\n        # Fallback narrative\n        if not narrative:\n            lines = []\n            lines.append(f\"**Patient Summary**\\nName: {name}\\nAge: {age}\\nGender: {gender}\\n\")\n            lines.append(f\"Reported symptoms: {', '.join(symptoms)}\\n\")\n\n            lines.append(\"**Considered Conditions**\")\n            for d, p in sorted_probs[:5]:\n                snippet = (disease_symptom_map.get(d, \"\") or \"\").split(\".\")[0][:180]\n                lines.append(f\"- **{d}** — {snippet}\")\n\n            lines.append(\"\\n**Reasoning Summary**\")\n            for d, p in sorted_probs[:5]:\n                kb = disease_symptom_map.get(d, \"\").lower()\n                matched = [s for s in symptoms if s.lower() in kb]\n                if matched:\n                    lines.append(f\"- {d}: matches {matched}.\")\n                else:\n                    lines.append(f\"- {d}: limited symptom overlap.\")\n\n            lines.append(\n                f\"\\n**Final Assessment:** Most likely diagnosis = **{top_d}** \"\n                f\"(confidence {top_p:.2f}).\"\n            )\n\n            narrative = \"\\n\".join(lines)\n\n        print(\"\\n================ FINAL DIAGNOSTIC REPORT ================\\n\")\n        print(textwrap.fill(narrative, 100))\n        print(\"\\n==========================================================\\n\")\n\n        return {\n            \"patient_meta\": {\"name\": name, \"age\": age, \"gender\": gender},\n            \"symptoms\": list(symptoms),\n            \"candidates\": [c for c, p in sorted_probs],\n            \"confidence\": {c: float(p) for c, p in sorted_probs},\n            \"final_diagnosis\": top_d,\n            \"narrative\": narrative\n        }\n\n    # -----------------------------------------------------\n    #  Main Consultation Loop\n    # -----------------------------------------------------\n    def consult(self, initial_message=None, name=None, age=None, gender=None, use_gemini=True):\n\n        # Ask demographics first\n        if name is None:\n            name = input(\"Patient Name: \").strip()\n        if age is None:\n            age = input(\"Age: \").strip()\n        if gender is None:\n            gender = input(\"Gender (M/F/Other): \").strip()\n\n        if initial_message is None:\n            initial_message = input(\"Describe your symptoms: \").strip()\n\n        # Red flag\n        if detect_red_flags(initial_message):\n            print(\"\\n⚠ RED FLAG: URGENT medical attention advised.\\n\")\n\n        # Seed via retriever\n        init_cands = hybrid_retrieve(initial_message, k=self.k)\n        seed = self._candidate_symptoms_pool(init_cands)[:3]\n        self.S = seed.copy()\n\n        inquiry_count = 0\n        prev_entropy = None\n\n        for _ in range(self.max_rounds):\n            candidates = hybrid_retrieve(\", \".join(self.S), k=self.k)\n            probs = self._score_candidates(candidates, self.S, age=age, gender=gender)\n\n            best_d = max(probs, key=probs.get)\n            best_p = probs[best_d]\n\n            print(f\"\\nROUND {_+1} — Best candidate: {best_d} ({best_p:.3f})\")\n\n            # Finalize only after 3+ questions\n            if inquiry_count >= self.MIN_INQUIRIES_BEFORE_DIAG and best_p >= self.conf_threshold:\n                return self.generate_final_cod_report(candidates, probs, self.S, name, age, gender, use_gemini)\n\n            # Entropy-based early stop (after 3 inquiries only)\n            entropy = -sum(p * math.log(p+1e-12) for p in probs.values())\n            if inquiry_count >= self.MIN_INQUIRIES_BEFORE_DIAG and prev_entropy is not None:\n                if (prev_entropy - entropy) < EARLY_STOPPING_ENTROPY_EPS:\n                    print(\"\\nEntropy plateau — finalizing early.\")\n                    return self.generate_final_cod_report(candidates, probs, self.S, name, age, gender, use_gemini)\n            prev_entropy = entropy\n\n            # Choose next symptom\n            st = self.choose_inquiry_by_entropy(candidates)\n            if not st:\n                return self.generate_final_cod_report(candidates, probs, self.S, name, age, gender, use_gemini)\n\n            inquiry_count += 1\n            self.asked.add(st.lower())\n\n            q = f\"Have you experienced '{st}'? (Yes/No)\"\n            ans = input(q + \": \")\n\n            if self.interpret_answer_as_bool(ans, st):\n                self.S.append(st)\n            else:\n                self.negatives.add(st.lower())\n\n        return self.generate_final_cod_report(candidates, probs, self.S, name, age, gender, use_gemini)\n\n\n\n# -----------------------------------------------------\n# Convenience wrapper\n# -----------------------------------------------------\ndef run_diagnosis(initial_message=None, name=None, age=None, gender=None, session=None, use_gemini=True):\n    if session is None:\n        session = InteractiveConsultationPaperV3(\n            model=globals().get(\"model\", None),\n            tokenizer=globals().get(\"tokenizer\", None),\n            llm_scoring=globals().get(\"ENABLE_LLM_SCORING\", False),\n            k_candidates=6,\n            confidence_threshold=0.75,\n            max_rounds=6,\n            save_transcript=True\n        )\n    return session.consult(initial_message, name, age, gender, use_gemini)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T05:33:43.128139Z","iopub.execute_input":"2025-12-08T05:33:43.128435Z","iopub.status.idle":"2025-12-08T05:33:43.161621Z","shell.execute_reply.started":"2025-12-08T05:33:43.128417Z","shell.execute_reply":"2025-12-08T05:33:43.161059Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"run_diagnosis()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T05:34:18.674090Z","iopub.execute_input":"2025-12-08T05:34:18.674667Z","iopub.status.idle":"2025-12-08T05:34:37.006467Z","shell.execute_reply.started":"2025-12-08T05:34:18.674643Z","shell.execute_reply":"2025-12-08T05:34:37.005683Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"Patient Name:  haasiwth\nAge:  18\nGender (M/F/Other):  M\nDescribe your symptoms:  i have cough and fever for 7 days\n"},{"name":"stdout","text":"\nROUND 1 — Best candidate: Pulmonary Infection (0.999)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Have you experienced 'dyspnea'? (Yes/No):  yes\n"},{"name":"stdout","text":"\nROUND 2 — Best candidate: Scrub Typhus Pneumonia (0.992)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Have you experienced 'persistent high fever'? (Yes/No):  yes\n"},{"name":"stdout","text":"\nROUND 3 — Best candidate: Influenza Virus Pneumonia (0.957)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Have you experienced 'rales'? (Yes/No):  no\n"},{"name":"stdout","text":"\nROUND 4 — Best candidate: Influenza Virus Pneumonia (0.957)\n\n================ FINAL DIAGNOSTIC REPORT ================\n\n**Patient Summary** Name: haasiwth Age: 18 Gender: M  Reported symptoms: high fever, cough,\nexpectoration, dyspnea, persistent high fever  **Considered Conditions** - **Influenza Virus\nPneumonia** — cough, fever with chills, sore throat, headache, chest pain, hemoptysis, myalgia,\npersistent high fever, dyspnea, cyanosis - **Pneumonia** — cough, expectoration, chest pain,\ndyspnea, fever - **Pulmonary Infection** — high fever, cough, expectoration - **Elderly Pulmonary\nTuberculosis** — rales, cough, expectoration, hemoptysis, dyspnea, persistent high fever -\n**Pediatric Viral Pneumonia** — persistent high fever, dyspnea, cyanosis, paroxysmal cough, and\nscanty hemoptysis sputum  **Reasoning Summary** - Influenza Virus Pneumonia: matches ['high fever',\n'cough', 'dyspnea', 'persistent high fever']. - Pneumonia: matches ['cough', 'expectoration',\n'dyspnea']. - Pulmonary Infection: matches ['high fever', 'cough', 'expectoration']. - Elderly\nPulmonary Tuberculosis: matches ['high fever', 'cough', 'expectoration', 'dyspnea', 'persistent high\nfever']. - Pediatric Viral Pneumonia: matches ['high fever', 'cough', 'dyspnea', 'persistent high\nfever'].  **Final Assessment:** Most likely diagnosis = **Influenza Virus Pneumonia** (confidence\n0.96).\n\n==========================================================\n\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"{'patient_meta': {'name': 'haasiwth', 'age': '18', 'gender': 'M'},\n 'symptoms': ['high fever',\n  'cough',\n  'expectoration',\n  'dyspnea',\n  'persistent high fever'],\n 'candidates': ['Influenza Virus Pneumonia',\n  'Pneumonia',\n  'Pulmonary Infection',\n  'Elderly Pulmonary Tuberculosis',\n  'Pediatric Viral Pneumonia',\n  'Aspiration Pneumonia in the Elderly'],\n 'confidence': {'Influenza Virus Pneumonia': 0.9565885010904421,\n  'Pneumonia': 0.028886469744809094,\n  'Pulmonary Infection': 0.014525029163792176,\n  'Elderly Pulmonary Tuberculosis': 1.5757554348766385e-19,\n  'Pediatric Viral Pneumonia': 1.5757554348766385e-19,\n  'Aspiration Pneumonia in the Elderly': 1.5757554348766385e-19},\n 'final_diagnosis': 'Influenza Virus Pneumonia',\n 'narrative': \"**Patient Summary**\\nName: haasiwth\\nAge: 18\\nGender: M\\n\\nReported symptoms: high fever, cough, expectoration, dyspnea, persistent high fever\\n\\n**Considered Conditions**\\n- **Influenza Virus Pneumonia** — cough, fever with chills, sore throat, headache, chest pain, hemoptysis, myalgia, persistent high fever, dyspnea, cyanosis\\n- **Pneumonia** — cough, expectoration, chest pain, dyspnea, fever\\n- **Pulmonary Infection** — high fever, cough, expectoration\\n- **Elderly Pulmonary Tuberculosis** — rales, cough, expectoration, hemoptysis, dyspnea, persistent high fever\\n- **Pediatric Viral Pneumonia** — persistent high fever, dyspnea, cyanosis, paroxysmal cough, and scanty hemoptysis sputum\\n\\n**Reasoning Summary**\\n- Influenza Virus Pneumonia: matches ['high fever', 'cough', 'dyspnea', 'persistent high fever'].\\n- Pneumonia: matches ['cough', 'expectoration', 'dyspnea'].\\n- Pulmonary Infection: matches ['high fever', 'cough', 'expectoration'].\\n- Elderly Pulmonary Tuberculosis: matches ['high fever', 'cough', 'expectoration', 'dyspnea', 'persistent high fever'].\\n- Pediatric Viral Pneumonia: matches ['high fever', 'cough', 'dyspnea', 'persistent high fever'].\\n\\n**Final Assessment:** Most likely diagnosis = **Influenza Virus Pneumonia** (confidence 0.96).\"}"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"# Save final PEFT LoRA model + tokenizer\nsave_dir = \"/kaggle/working/diagnosisgpt_v3_3_model\"\n\nos.makedirs(save_dir, exist_ok=True)\n\nmodel.save_pretrained(save_dir)\ntokenizer.save_pretrained(save_dir)\n\nprint(\"Model saved to:\", save_dir)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T05:01:24.205755Z","iopub.execute_input":"2025-12-08T05:01:24.206308Z","iopub.status.idle":"2025-12-08T05:01:24.614967Z","shell.execute_reply.started":"2025-12-08T05:01:24.206279Z","shell.execute_reply":"2025-12-08T05:01:24.614255Z"}},"outputs":[{"name":"stdout","text":"Model saved to: /kaggle/working/diagnosisgpt_v3_3_model\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"MODEL_DIR = \"diagnosis_gpt_v3_3/model_final\"\nprint(\"Saved model directory:\", MODEL_DIR)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T05:42:16.726131Z","iopub.execute_input":"2025-12-08T05:42:16.726652Z","iopub.status.idle":"2025-12-08T05:42:16.730729Z","shell.execute_reply.started":"2025-12-08T05:42:16.726626Z","shell.execute_reply":"2025-12-08T05:42:16.730173Z"}},"outputs":[{"name":"stdout","text":"Saved model directory: diagnosis_gpt_v3_3/model_final\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel.save_pretrained(MODEL_DIR)\ntokenizer.save_pretrained(MODEL_DIR)\n\nprint(\"✔ Model + tokenizer saved to:\", MODEL_DIR)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T05:42:24.318842Z","iopub.execute_input":"2025-12-08T05:42:24.319907Z","iopub.status.idle":"2025-12-08T05:42:24.847851Z","shell.execute_reply.started":"2025-12-08T05:42:24.319850Z","shell.execute_reply":"2025-12-08T05:42:24.847022Z"}},"outputs":[{"name":"stdout","text":"✔ Model + tokenizer saved to: diagnosis_gpt_v3_3/model_final\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import shutil\n\nZIP_PATH = \"diagnosis_gpt_v3_3_model.zip\"\nshutil.make_archive(\"diagnosis_gpt_v3_3_model\", \"zip\", MODEL_DIR)\n\nprint(\"✔ ZIP created:\", ZIP_PATH)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T05:42:32.879858Z","iopub.execute_input":"2025-12-08T05:42:32.880732Z","iopub.status.idle":"2025-12-08T05:42:34.252351Z","shell.execute_reply.started":"2025-12-08T05:42:32.880705Z","shell.execute_reply":"2025-12-08T05:42:34.251691Z"}},"outputs":[{"name":"stdout","text":"✔ ZIP created: diagnosis_gpt_v3_3_model.zip\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install datasets tqdm\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-08T06:41:26.378Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\n\ndx = load_dataset(\"FreedomIntelligence/DxBench\", \"en\")\ntest_data = dx[\"test\"] if \"test\" in dx else dx[\"train\"]     # if no test split\nprint(\"Samples:\", len(test_data))\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-08T06:41:26.379Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_symptom_text(sample):\n    pos = [s[0] for s in sample[\"explicit_symptoms\"] if s[1].lower()==\"true\"]\n    pos += [s[0] for s in sample[\"implicit_symptoms\"] if s[1].lower()==\"true\"]\n    text = \"I have \" + \", \".join(pos)\n    return text\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-08T06:41:26.379Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}